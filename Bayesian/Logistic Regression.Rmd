---
title: "Logistic Regression"
author: "James Rigby"
date: "10/26/2019"
output: html_document
---

In this tutorial, I am going to describe the inner working of logistic regression. The first portion of the post is going to break the logistic model into small elements, writing a function for each of the chunks. This is meant to illustrate how logistic regression is actually estimated. The second part is going to approach logistic regression from a bayesian perspective and illustrate how Bayesian learning can be implemented to create an online machine learning tool. The final portion of this paper is going to demonstrate how to speed up bayesian analyses using approximations to the posterior. This post is going to be heavy on statistical theory, but I will try to point out the sections that those primarly conserned with application should be aware of in the section headers. 


## Problem Setup
Your consulting firm was recently hired to build a system that integrates with the company's Buisiness Intelligence (BI) software to understand the factors that influence employee recruitment. The primary critereon of interest is job acceptances - candidates decisions to accept or reject an offer made my the organization. You decide to begin this project by exploring a logit model (logistic regression), because you hear that it is easily scaleable and able to be developed into an active learning. 

The BI platform provides you with information from recent job analyses, salary market surveys, job market research, and the benefits packages that were offered to the employee. I print the head of the data frame below. 


# Simulating Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE, echo = FALSE}
library(tidyverse)
library(decisionr)
library(knitr)
#########################################
######## Generating Jobs ################
#########################################

# Setting the random seed
set.seed(1234)

#Initializing Vector of Zeros
Intercept<-flexibility<-difficulty<-job_alt<-salary<-market_sal<-vector(mode = "numeric", 100)

# Populating every other vector with job characteristics
# Each vector contains information on 50 jobs, but since individuals are choosing between accepting a job and not accepting a job, there is an additional 50 rows for individuals not accepting a job
# Jobs vary on four dimensions job_alt, difficulty, flexibility and salary
salary[seq(from = 2, to = 100, by = 2)]<-rnorm(50, 30, 5)
job_alt[seq(from = 2, to = 100, by = 2)]<-sample(1:4, 50, replace = TRUE, prob = c(.5, .3, .1, .1))
difficulty[seq(from = 2, to = 100, by = 2)]<-sample(1:4, 50, replace = TRUE, prob = c(.3, .5, .1, .1))
flexibility[seq(from = 2, to = 100, by = 2)]<-sample(1:4, 50, replace = TRUE, prob = c(.7, .1, .07, .13 ))
Intercept[seq(from = 2, to = 100, by = 2)]<-1
market_sal[seq(from = 2, to = 100, by = 2)]<-salary[seq(from = 2, to = 100, by = 2)]+rnorm(n = 50, mean = 0, sd = 5)


# Thus the job feature matrix has one row associated with the job and one row associated with not the job
design_df<-data.frame(BLOCK = rep(1:50, each = 2), 
                              QES = rep(1:50, each = 2),
                              alt = rep(0:1, times = 50),
                              Intercept = Intercept,
                              cost = salary,
                              job_alt = job_alt,
                              difficulty = difficulty,
                              flexibility = flexibility,
                              market_sal = market_sal)%>%
          mutate(sal_by_market_sal = cost*market_sal)
```

```{r, message=FALSE, warning=FALSE, echo = FALSE}
#################################################
######## Simulating person preferences ##########
#################################################

# Participants have their own preferences associated with each of these positons, for now we will simulate them as being fixed (i.e., an single preference parameter adequetly summarises all people)
# We simulate these preference distributions below. 
people<-sim_people(fixed_effects = c("Intercept" = 0, "cost" = .3, "job_alt" = 1, "flexibility" = .3, "difficulty"  = -2, "market_sal" = -.15, sal_by_market_sal = -.003), n_people = 250, n_blocks = 50, p_blocks = rep(.02, 50))


#########################################
######## Simulating decisions ###########
#########################################
# The following code chunch has the simulated people (i.e., job applicants) choose between accepting and rejecting the simulated job offers
# I then organize the data frame a bit more so that the meaning of variables is clear

dec<-sim_dec(design = design_df, people = people)%>%
  ungroup()%>%
  filter(alt == 1)%>%
  select(job_id = BLOCK, applicant_id = id, salary = cost,  job_alt:market_sal, decision)

head(dec)%>%
  kable()%>%
  kableExtra::kable_styling(bootstrap_options = "striped")
```

## Approaching Classification from a Linear Perspective
Why is linear regression not the most appropriate technique for categorical outcomes? If we code job acceptance as 0 or 1 we can predict it using a linear regression model. Furthermore, if the goal of our analysis make predictions about those who accept a job and those who do not accept a job we could create a decision boundary. For example, we could classify job applicants with a predicted value above .5 as accepters and classify those with a score below .5 and rejecters. 

```{r}
p<-dec%>%
  ggplot(aes(x = salary, y = decision))+
  geom_smooth(method = "lm", se = FALSE, color = "red", fullrange = TRUE)+
  geom_point()+
  geom_vline(xintercept = 25.80, lty = 3)+
  labs(title = "Linear Approach to Classification", 
       x = "Salary", 
       y = "Predicted Values (1 = Accepted Job Offer)")+
  scale_y_continuous(breaks = seq(0, 1, by = .1))

plotly::ggplotly(p)

```
What does .5 mean though? Some people may be tempted to say that .5 is the probability that an applicant accepts a job offer. That interpretation is incorrect. Imagine that we tried to generalize this model to make predictions for a new set of jobs. The salary for these positions are a little bit higher. When we make predictions for the new point (depicted as a cross). The predicted value is 1.08. Given that probabilities are bounded by 0 and 1, this is cleary not a probability. In fact, the predicted values are relatively meaningless.

```{r}
p<-p+
   geom_point(data = data.frame(salary = 55, decision = 1), aes(x = salary, y = decision), shape = 3)+
  scale_y_continuous(breaks = seq(0, 1.2, by = .1))

plotly::ggplotly(p)
```



## Logisic Regression: A Probablistic Approach to Classification

We can still approach classification from a linear perspective, however, we need to transform the function to accomidate the binary data. Logistic regression does this by assuming the log odds of the categorial outcome is linearly related to the independent variables. 


$$log\frac {p(y=1)}{p(y=0)} = X\beta+e$$

We can rearrange this function to depict the model in probablistic terms. 

$$\frac {p(y=1)}{p(y=0)}= e^{X\beta+e}$$
$$\frac {p(y=1)}{1-p(y=1)}= e^{X\beta+e}$$
$$p(y=1)= (e^{X\beta+e}-e^{X\beta+e}p(y=1)) $$
$$p(y=1)+e^{X\beta+e}p(y=1)= (e^{X\beta+e}) $$
$$p(y=1)(1+e^{X\beta+e})= (e^{X\beta+e}) $$
$$p(y=1) = \frac{(e^{X\beta+e})}{(1+e^{X\beta+e})} $$
This can be written in a slightly more compact form

$$p(y=1) = \frac{1}{(1+e^{-(X\beta+e)})}$$
Notice that there is still a linear component $X\beta + e$, however it is transformed to ensure that the function is bounded by 0 and 1. 


How does this new function look? Unlike the linear classifier (red line) the logistic model's (blue line) predicted values can be directly interpreted as the probability of an employee accepting a job given the predictors. 

```{r}

p<-p+
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), 
              fullrange = TRUE, 
              se = FALSE, 
              color = "blue")+
  scale_y_continuous(breaks = seq(0, 
                                  1.2, 
                                  by = .1))+
  labs(title = "Comparing Linear and Logistic Approaches to Classification")

plotly::ggplotly(p)

```

Zooming out provides a better illustration of how the two functions differ. The logit model assymptotes at 0 and 1 while the linear model is, well, linear. Interstingly, the decision boundaries for the two models seem aligns at a .5 threshold. However, if we adjusted the decion boundary above or below .5, the two models would diverge. 

```{r}
p<-p+
  lims(x = c(-20, 70))+
  geom_hline(yintercept = 1, lty = 3)+
  geom_hline(yintercept = 0, lty = 3)

p$layers[[3]]<-NULL

plotly::ggplotly(p)
```


Lets define the functional form of the logistic regression. To make it's relation to the linear model explicit, I defined a function called `v` which is the estimate of the exponentiated portion of the model (i.e., $X\beta$). I then define a function called `p_hat` which transforms the linear output to estimate probabilities. 


```{r}
# Linear combination of predictors
v<-function(X, beta){
  v<-X%*%beta
  v
}

# Estimated Probability through sigmoid transformation
p_hat<-function(v){

  p<-1/(1+exp(-v))
  
  p
}
```

## Maximum Likelihood Estimation

Just like linear regression selects the regression coefficients that minimize the sum of squared error, logistic regression has an objective criterea, or objective function. Logistic regression is fit using maximum likelihood estimation. Broadly speaking, maximum likelihood tries to find the parameters ($\beta$) that maximize the probability of observing the data that was observed (i.e, $p(y|\beta, X)$). In practice, the negative log likelihood is typically minimized, but maximium likelihood sounds better than negative minimum log likelihood. The negative log likelihood function for logistic regression is written below.

$$L(\beta) = -\sum_{i=1}^n\ y_i\ ln(\ \hat p_i(y = 1)\ )+(1-y_i)\ ln(\ 1-\hat p_i(y = 1)\ )$$

What does the likelihood function mean? Let's break the function into two pieces by spliting it at the plus sign. 

**Part 1**

$$y_i\ ln(\ \hat p_i(y = 1)\ )$$
This portion of the likelihood function multiplies the oberved outcome ($y_i$) by the log of the expected probability. As probabilities fall between 0 and 1 and the log of numbers closer to 0 become more negative, this portion of the likelihood is largest when 1) the predicted probability is high and 2) the observed outcome was one. In other words, this portion of the equation is the largest when observations are correctly classified as belonging to class 1. Observations that are 0 do not contribute to this part of the likelihood. The plot below illustrates how this function behaves across classes as the predicted probability increases. 

```{r}
expand.grid(p_hat = seq(.001, 1, .001), y_i = c(0,1))%>%
  mutate(likelihood = y_i*log(p_hat),
         y_i = as.factor(y_i))%>%
  ggplot(aes(x = p_hat, y = likelihood, color = y_i))+
   geom_line()+
  labs(x = "Predicted Probability", 
       y = "Part 1 Likelihood",
       title = "Illustration of Part 1 of Likelihood Function")+
  guides(color = guide_legend("Observed Y"))+
  scale_color_manual(values = c("red", "blue"))
```

**Part 2**

$$(1-y_i)\ ln(\ 1-\hat p_i(y = 1)\ )$$

The second half of the likelihood formula follows the same logic. The major differences is that it focuses on encorporating the other class. This portion of the likelihood is maximized when 1) the predicted probability is closest to 0 and 2) the observed class was 0. Thus, this portion is maximized when the model accurately predicts class 0. The plot below depicts this function. 

```{r}
expand.grid(p_hat = seq(0, .999, .001), y_i = c(0,1))%>%
  mutate(likelihood = (1-y_i)*(log(1-p_hat)),
         y_i = as.factor(y_i))%>%
  ggplot(aes(x = p_hat, y = likelihood, color = y_i))+
   geom_line()+
  labs(x = "Predicted Probability", 
       y = "Part 1 Likelihood",
       title = "Illustration of Part 1 of Likelihood Function")+
  guides(color = guide_legend("Observed Y"))+
  scale_color_manual(values = c("red", "blue"))
```


Taken together, the likelihood function is the most positive for the $\beta$ that do a the best job discriminating between the classes. The function below writes the likelihood function. Its sole arguments are the observed values `y` and the predicted probabilities `p` which can be generated by `p_hat()`. 


```{r}
# Implementation of the likelihood
ll<-function(y, p){
  -sum(y*log(p)+(1-y)*log(1-p))
}
```

## Newtonian Optimization to Solve for Beta
How do we solve for the $\beta$ that maximize the likelihood of our observed data? One method is to use Newtonian optimization to solve for the maximum of our likelihood function. Newtonian methods rely on a second order Taylor expansion that approximates the likelihood function. So long as the function is second differentiable we can iteratively update our estimates for $\beta$ using the following equation:

$$\beta_{new}= \beta_{old}-H_\beta^{-1}\Delta\beta$$

where $\Delta\beta$ is the vector of first derivatives of the likelihood function with respect to $\beta$ and $H_\beta$ is equal to the the hessian, or matrix of second derivatives, of the likelihood function with respect to the regression coefficients. 

This function sets the derivatives of the quadratic approximation to 0, therby solving for the approximation's minimum or maximum.

To visually illustrate what newtonian optimization does, Let's try to optimize the following equation. 

$y = .25x^2$$

```{r}
data.frame(x = -20:20)%>%
  mutate(y = .25*x^2)%>%
  ggplot(aes(x = x, y = y))+
  geom_line()
```

To begin optimizing, we need to initalize the function by picking starting values. Lets pick a point thats really far off from the stationary point (the minimum).

```{r}
obj_function<-function(x){
  .25*x^2
}

data.frame(x = -30:30)%>%
  mutate(y = obj_function(x))%>%
  ggplot(aes(x = x, y = y))+
  geom_line()+
  geom_point(aes(x = -20, y = obj_function(-20)))
```

The derivative of the objective function in this example is equal to $.5x$ while the second derivative is equal to $.5$. To verify my calculations are correct, I plot the objective below. As a reminder the first derivative is equal to the tangent line, or the instintaneous slope at a point $x_n$. 

```{r}
obj_function<-function(x){
  .25*x^2
}

data.frame(x = -30:30)%>%
  mutate(y = obj_function(x))%>%
  ggplot(aes(x = x, y = y))+
  geom_line()+  
  geom_segment(x = -20-3, xend = -20+3, y = obj_function(-20)+10*3, yend = obj_function(-20)-10*3, color = "red", size = 1.07)+
  geom_point(aes(x = -20, y = obj_function(-20)))

```

Let's take a look at the taylor expansion of this equation about our starting point.  

$$f_T(x)= f(x_n)+f'(x_n) \Delta x+ \frac{1}{2} f''(x_n) \Delta x^2$$

We can plug in our starting values and the derivatives to define and plot the quadratic approximation. 

$$f_T(-20) = 100 + (-10 \times (x+20)) + \frac{1}{2} (.5 \times (x+20)^2)$$

Plotting this function over the objective function reveals that it is a fantastic approximation to the objective function, which may not be supprising given that true form of the objective is quadratic. 

```{r}
taylor_exp_neg_20<-function(x){
 obj_function(-20)+ (x+20)* -10+ 1/2* .5* (x+20)^2
}

data.frame(x = -30:30)%>%
  mutate(y = obj_function(x),
         taylor = taylor_exp_neg_20(x))%>%
  ggplot(aes(x = x, y = y))+
  geom_line()+
  geom_line(aes(x = x, y = taylor), color = "red", lty = 2, size = 1.07)+
  geom_point(aes(x = -20, y = obj_function(-20)))
```

We can apply the update rule by applying the following formula

$$x_{new} = x_{old}-\frac{\Delta x}{\Delta \Delta x}$$
$$x_{new} = -20-\frac{-10}{.5}$$

Given the perfect congruence between the taylor expansion and the objective function, this optimization problem converges in one iteration. This only occurs when the function you are trying to maximize or minimize is perfectly represented by the taylor expansion. Unfortunately, the likelihood function requires multiple taylor expansions to converge but this isn't a huge deal. We just need to know a little bit of programming to get the job done!

```{r}
data.frame(x = -30:30)%>%
  mutate(y = obj_function(x),
         taylor = taylor_exp_neg_20(x))%>%
  ggplot(aes(x = x, y = y))+
  geom_line()+
  geom_line(aes(x = x, y = taylor), color = "red", lty = 2, size = 1.07)+
  geom_point(aes(x = -20, y = obj_function(-20)))+
  geom_segment(aes(x = -20, xend = -20, y = obj_function(-20), yend = obj_function(-20+10/.5)), lty = 2, color = "red")+
  geom_segment(aes(x = -20, xend = -20+10/.5, y = 0, yend = 0), color = "red", lty = 2)+
  geom_point(aes(x = 0, y = 0), shape = 2)
```


Because the likelihood in logistic regression is second differentable, we can apply newtonian methods! The first derivative of likelihood function can be estimated using the following formula

$$\Delta\beta = -X^\top (y-p(y=1))$$
I implement this function below. 


```{r}
deriv<-function(X, y, p){
  t(X)%*%(y-p)
}
```



The second derivative has quite a special place in psychological assessment. This is because of its close relationship with the information matrix, a matrix that comes up quite frequently in IRT models (The information matrix is simply the negative of the hessian). It's scalar representation may seem a little bit tricky, but the matrix calculations (how I implement my R code below) is actually quite easy! I'll show both derivations below. The scalar representation can be calculated as follows:

$$\frac{\delta^2L(\beta)}{\delta\beta_j\delta\beta_l} = \sum_{i = 1}^n\ x_{ij}\ x_{il}\ p_i(y = 1)\ (1-p_i(y = 1))$$

This solves for element $j, l$ of the hessian matrix. 

The entire hessian can be solved using the following matrix algebra. 

$$H_{\beta} = X^\top S X$$
where S is a diagnol matrix with $p(y=1)(1-p(y=1))$ along the diagonal. 

```{r}
# Diaganol Matrix S
S_mat<-function(p){
  var<-p*(1-p)
  dim<-length(var)
  s<-matrix(0, nrow = dim, ncol = dim)

  diag(s)<-var
  
  s
}


# Calculate Information Matrix
second_deriv<-function(X, S){
  t(X)%*%S%*%X
}
```

## Iteratively Reweighted Least Squares
If you read many methodological papers on logistic regression it is only a matter of time before you come across the term "Iteratively Reweighted Least Squares". An interesting property of the Newton step described above is that it simplifies to another optimization method called iteratively reweighted least squares. We could really get into the weeds with this method, but there is a fantastic post on stacks exchange that explains its derivation and relation to newtonian methods. You can find the link here [here](https://stats.stackexchange.com/questions/344309/why-using-newtons-method-for-logistic-regression-optimization-is-called-iterati). The author, jld (Joseph) has a fantastic, by the way [blog](https://jld-stats.com/author/josephldeutsch/).

Assuming that those interested followed up by reading the blog post, I will just show the update rule below 

Formulaically, the iterative least squares step is equal:

$$\beta_{new} = H_\beta^{-1}X^\top S z$$
where z is defined as $X \beta+S^{-1}(y-p(y=1))$.

Those who read the blog post may be confused by the notation, but this derivation is identical to that written by jld and my representation based on Hastie, Tibshirani, and Friedman's (2001) derivation in Elements of Statistical Learning. Both are accurate but use different notational conventions! The code below defines the nessissary functions.

```{r}
# Define z
z<-function(X, theta, S, y, p){
  X%*%theta+solve(S)%*%(y-p)
}
```

```{r}
# Update the regression weights
update_beta<-function(i_mat, X, S, z){
  
 (solve(-i_mat))%*%t(X)%*%S%*%z

  }
```


## When do we stop?
Iterative methods require some stopping criterea. The default for some software proprietary statistical packages is the relative gradient convergence critereon. To ensure that the model doesn't get stuck in a perpetual loop, I also include a maximum iterations option as a safe gaurd, but typically models converge quickly. 


```{r}
relative_grad_crit<-function(deriv, i_mat, ll){
  abs((t(deriv)%*%solve(i_mat)%*%deriv)/(abs(ll)+1e-6))
}
```

## Hypothesis Testing

If you are interested in making inferences about the relationship between the predictors and outcome, deriving the information matrix the final piece to the puzzle. The information matrix is important because, based on asymptotic normal theory $\hat \beta \sim MVN(\beta, \hat\Sigma_\beta)$ where $\Sigma_\beta$ is equal to the inverse of information matrix. This means that the standard erros of beta, which are used in Wald's tests for significance, are equal to the square root of the diagonal of $\Sigma_{\beta}$.  

```{r}
information_mat<-function(sec_deriv){
  -sec_deriv
}
```

## Putting it all together
Below I create a function that pulls together all of the pieces
```{r}
log_reg<-function(X, y, max_iter = 20, tol = 1e-8){
  
  # Initalize the number of iterations so max_iter will serve as a stopping critereon
  i<-1

  # Provide starting values for the regression weights (set them to 0)
  theta<-rep(0, times = ncol(X))

  # Solve all components of the model for starting values
  # These just implement our functions that were defined above
  
  # Solves for the predicted probability given starting values
  v_i<-v(X = X, beta = theta)
  p<-p_hat(v = v_i)
  
  # Solves for the first and second derivaties of likelihood function given start values
  deriv_i<-deriv(X, y, p)
  S<-S_mat(p = p)
  s_deriv<-second_deriv(X = X, S = S)
  i_mat<-information_mat(sec_deriv = s_deriv)
  
  # Initializes the stopping critereon
  grad_crit<-relative_grad_crit(deriv_i, i_mat, ll(y, p))
  
  # Implementing Iteratively Reweighted Least Squares
  # Runs until stopping critereon is reached
  while(i<max_iter&grad_crit>tol){
    
  # Update beta using IRLS step
  z_i<-z(X = X, theta = theta, S = S, y = y, p = p)
  theta<-update_beta(i_mat = i_mat, X = X, S = S, z = z_i)
  
  # Solve for VI and P with new values
  v_i<-v(X = X, beta = theta)
  p<-p_hat(v = v_i)
  
  # Solves for the first and second derivaties
  deriv_i<-deriv(X, y, p)
  S<-S_mat(p = p)
  s_deriv<-second_deriv(X = X, S = S)
  i_mat<-information_mat(sec_deriv = s_deriv)
  
  # update ll
  ll_new<-ll(y = y, p = p)
  
  # Return a message with the iteration value
  message("Iteration: ", i, ll_new)
  
  # move one iteration forward
  i<-i+1
  
  # Calculate stoping critereon
  grad_crit<-relative_grad_crit(deriv_i, i_mat, ll(y, p))
}

# Calculate final log likelihood and betas
final_ll<-ll(y = y, p = p)
final_beta<- theta

# Format the betas
names(final_beta)<-c(colnames(X))

# Return the Values
list(betas = final_beta, 
     LL = -final_ll, # Returns negative final ll because ll is producing negative ll
     vcov = -solve(i_mat))
}
```


# Giving the Model A Try
```{r}
# Prep Matrices
X<-dec[3:7]
y<-dec["decision"]

X<-cbind(1, X)
X<-as.matrix(X)
y<-as.matrix(y)

colnames(X)[1]<-"(Intercept)"

our_reg<-log_reg(X = X, y = y, max_iter = 100)
```

# How Does Our Model Compare with glm?
Lets fit a model using the standard glm function and compare it to our results. Below, I output the coefficients and standard errors from both model. Not bad, right?


```{r}
their_reg<-glm(decision~., data = dec%>%select(salary:decision), family = "binomial")

data.frame(our_coefs = as.vector(our_reg$betas),
           our_se = sqrt(diag(our_reg$vcov)),
           their_coefs = coef(their_reg),
           their_se = sqrt(diag(vcov(their_reg))))%>%
  kable(format = "html")%>%
  kableExtra::kable_styling(bootstrap_options = "striped")


logLik(their_reg)
our_reg$LL
```

## Next Time
Come back for part two of this post where we learn how to implement a couple of different online learning techniques including online learning and stochastic gradient descent. 





## A Quick Review
  In my last post, I illustrated how to write a function for fitting logistic regression, a common model for classifying dichotimous categorical outcomes. The model predicted the probability that y equaled class 1 ($p(y = 1|X)$) by applying a sigmoid transformation to the linear combination of predictors($\frac{1}{(1+e^{-(X\beta)})}$). Regression weights were chosen such that the likelihood of the observed data given beta ($p(y|\beta)$) was maximized. 

## What can be improved on?
  Our maximum likelihood implementation is nice, but it has some significant shortcommings "as is". First, traditional maximum likelihood logistic regression does not take into account prior information. Ignoring prior information makes it difficult to refit the model as new batches of data come in. Consider the motivating example from the previous post. Our goal was to build a classification system to predict the probability of an employee accepting a job offer. As hiring cycles begin and end, users may want to update their model with each new batch of data that comes in! Our function as is, cannot accommidate updates and must be refit using all available data. Storing this amount of data can lead to extensive (and unnessary) database management costs. While this may not be a big deal for small organizations, when working with large organizations this could become a sizeable cost! It would be nice if we could simply store some aspects of the model and then update our model as new information comes in.

  Not only do traditional maximum likelihood estimates fail to account for prior information, it approaches uncertainty in parameter estimates in a rather unrefined way. Maximum likelihood is rooted in frequentist notions of probability. In short frequentists approach probability as the expected value of a series of infinite repeated trials. In other words, frequentists make the assumption that there is some true parameter $\theta$, that generates the observed data. They use the expected values of the theta (often estimated through maximum likihood) to make their best guess about the "true parameter". In some cases this is an extremely useful approach to probability! In others, such as one off or rare events where repeated trials are not possible, this interpretation doesn't map to reality.
  
  This approach to probability makes explaining some aspects of frequentist statistics sometimes difficult. For example, frequentists quantify their uncertainty about parameter estimates using confidence intervals which have a rather unnatural interpretaton. These intervals can only be interpreted in terms of repeated trials. Imagine I were to quantify uncertainty in an estimate of the expected number of heads in a set of 10 coin tosses (CI95% = [3, 7]). This interval cannot be interpreted as the range of values that we are 95% certain the true average number of heads falls in. This range can only be interpreted in the context of repeated expiriments and calculations of confidence intervals. In other words, a 95% CI is the range of values that, if the experiment were repeated and CIs were calculated a large number of times (say 1000), would contain the true mean 95% of the time.

## What does a bayesian perspective contribute?

  The bayesian perspective addresses both of these short commings by defining and estimating a full probability model. Some may recognize the below formula as Bayes rule.

$$p(A|B) = \frac{p(B|A)p(A)}{p(B)}$$

  Let's put things in terms of our logistic regression model. We have a set of parameters, $\beta$, our data, $D$, and a set of prior beliefs about beta $\beta_{prior}$. We are interested in describing the density of $p(\beta|D)$, which is often referred to as the **posterior distribution**. However, the maximum likelihood approach we implemented previously focuses exclusively on $p(D|\beta)$. We can implement bayes rule in this scenario by modeling: 

$$p(\beta|D) = \frac{p(D|\beta)p(\beta|\beta_{prior})}{p(D)}$$

  Lucky for us there are some familiar terms! We know that the likelihood formula above is equivalent to the $p(D|\beta)$. The term $p(\beta|\beta_{prior})$ is simply a probability density or the prior evaluated at $\beta$. For example, if we use a normal prior on the regression coefficients, $p(\beta|\beta_{prior})$ could be solved for using `dmvnorm(beta, mu_beta_prior, Sigma_beta_prior)`. Plugging in the likelihood for $P(D|\beta)$ and using our new understanding of $P(\beta|\beta_{prior})$ we are left with $p(D)$. 

$$p(\beta|D) = \frac{L(\beta)p(\beta|\beta_{prior})}{p(D)}$$

  $p(D)$ is a scaling constant that ensures the product of the likelihood an prior terms integrate to 1. Unforuntately in our case there is no analytic solution for $P(D)$. This is because when we apply the sigmoid transformation to the linear model, we remove the possibility of finding conjugate priors (see [here](https://en.wikipedia.org/wiki/Conjugate_prior)). Instead of using formulaic methods we must rely on true bayesian methods that simulate the posterior distribution (Hamiltonian Markov Chain Monte Carlo) or approximate the posterior. 
  
  So what does this new approach buy us? Well, for starters, we can account for prior information. Furthermore, the posteriors of the model can often be summarised by distributional parameters making the storage requirements for the model very small. Iteratively updating priors and posteriors as new information comes in allows our algorithms to learn and update over time. Finally, by conditioning $\beta$ on prior information, we can actually talk about probability in terms that are much easier to explain to stakeholders. For example, the 5th and 95th percentiles of the posterior can directly be interpreted as the range of values for which we are 95% certain the "true" parameter lies (assuming you are willing to accept there is a "true" parameter).

## Markov Chain Monte Carlo

   Given that there is no closed form solution for the posterior of logistic regression, how can we capitalize on the benefits of Bayesian regression? Markov Chain Monte Carlos are a class of methods used to simulate the posterior distribution of a set of parameters $\theta$. The intution behind them is relatively simple. In cases where there is not an easy way to calculate a normalizing constant, we can iteratively simulate draws from the distribution of $\theta$. 
   
   To simulate these draws, we first pick a starting value for our algorithm. Next, we propose a new value of theta by adding a random number to the start point. If that value is more likely than the starting point, given the data we observed and our priors, we accept this new theta and store it as a possible value. If the new theta's likelihood is less than the starting point we (*probably*) reject this as being a possible value of theta. We then add another random number to the most recent accepted theta and repeat the process. After thousands of iterations we can look at the distribution of theta and get a good idea at the distribution of possible values for our parameters of interest. I include quasi-code down below to better illustrate the algrothim I just described. This is, infact, the Metropolis algorithim and gives a good intuition about the purpose of more advanced Markov Chain Monte Carlo methods. 

```{null}
Metropolis Algorithm
Define y, X, prior_theta, theta_start, maxit;

Initialize theta = theta_start;
Initialize p_theta = p(y | theta_start) * p(theta_start | prior_theta);
Initialize counter = 1;
Initialize post_dist = vector(mode = "numeric");

for t in 1 to maxit;
  
  proposal = theta + rnorm(1);
  
  p_proposal = p(y | proposal) * p(proposal | prior_theta)

  ratio = p_propoal / p_theta;
  
  if ratio >= 1;
    accept_proposal = TRUE;
  else;
    accept_proposal = as.logical(rbinom(n = 1, size = 1, prob = ratio));
  end; 
  
  if accept_proposal = TRUE;
    theta =  proposal;
    p_theta = p_proposal;
    post_dist[counter] = theta;
    counter += 1
  end;
end;

return post_dist;
```

While I do love creating algorithms from scratch, creating an efficient Markov Chain Monte Carlo function that works in a variety of cases ends up being a TON of work. The issues stems from the jump distribution (the distribution from which proposal thetas are selected). Sadly, for this chapter, I am going to rely on the `brms` package to get the job done. I say "sadly", but `brms` is a fantastic package that I would recommend to anyone! For those of you who like devloping from scratch algorithms, I will show a speedy alternative to MCMC in the section that follows. 

I am going to be working with a similar data frame as the last post, although I am cutting down the size in order to decrease the models' run times. MCMC is pretty slow furthermore I want to illustrate how bayesian learning can be implemented. To further faciliate this process and clean up the code I define a few helper functions below. These aren't generalizable but do clean up the code. 

```{r, eval = FALSE}
# Simulation Parameters
fe<-c("Intercept" = 0, "salary" = .3, "job_alt" = 1, "flexibility" = .3, "difficulty"  = -2, "market_sal" = -.15)
```

```{r, echo = FALSE, message = FALSE, warning=FALSE}
set.seed(1234)
library(tidyverse)
library(decisionr)

fe<-c("Intercept" = 0, "cost" = .3, "job_alt" = 1, "flexibility" = .3, "difficulty"  = -2, "market_sal" = -.15)

fe_data<-t(fe)%>%
  as.data.frame()%>%
  rename(salary = cost)%>%
  gather(key = var, value = value)

#Initializing Vector of Zeros
Intercept<-flexibility<-difficulty<-job_alt<-salary<-market_sal<-vector(mode = "numeric", 100)

# Populating every other vector with job characteristics
# Each vector contains information on 50 jobs, but since individuals are choosing between accepting a job and not accepting a job, there is an additional 50 rows for individuals not accepting a job
# Jobs vary on four dimensions job_alt, difficulty, flexibility and salary
salary[seq(from = 2, to = 100, by = 2)]<-rnorm(50, 30, 5)
job_alt[seq(from = 2, to = 100, by = 2)]<-sample(1:4, 50, replace = TRUE, prob = c(.5, .3, .1, .1))
difficulty[seq(from = 2, to = 100, by = 2)]<-sample(1:4, 50, replace = TRUE, prob = c(.3, .5, .1, .1))
flexibility[seq(from = 2, to = 100, by = 2)]<-sample(1:4, 50, replace = TRUE, prob = c(.7, .1, .07, .13 ))
Intercept[seq(from = 2, to = 100, by = 2)]<-1
market_sal[seq(from = 2, to = 100, by = 2)]<-salary[seq(from = 2, to = 100, by = 2)]+rnorm(n = 50, mean = 0, sd = 5)


# Thus the job feature matrix has one row associated with the job and one row associated with not the job
design_df<-data.frame(BLOCK = rep(1:50, each = 2), 
                              QES = rep(1:50, each = 2),
                              alt = rep(0:1, times = 50),
                              Intercept = Intercept,
                              cost = salary,
                              job_alt = job_alt,
                              difficulty = difficulty,
                              flexibility = flexibility,
                              market_sal = market_sal)%>%
  mutate_at(vars(cost, job_alt, difficulty, flexibility, market_sal), ~(.-mean(.))/sd(.))

data_list<-map(1:10, function(x){
  # Simulate People
people<-suppressMessages(
  sim_people(fixed_effects = fe, 
             n_people = 50, 
             n_blocks = 50, 
             p_blocks = rep(.02, 50)
             )
  )

# Simulate Decisions
dec<-suppressMessages(
  sim_dec(design = design_df, 
          people = people)
  )%>%
  rename(salary = cost)%>%
  ungroup()%>%
  filter(alt == 1)

dec
}
)
dec<-data_list[[1]]
```

```{r}
# Helper functions for summarising the posterior distribution
post_summary<-function(model){
  model%>%
  spread_draws(b_Intercept, b_salary, b_job_alt, b_difficulty, b_flexibility, b_market_sal)%>%
  select(b_Intercept:b_market_sal)%>%
  rename_all(.funs = ~str_remove(., "^b_"))%>%
  gather(key = var, value = value)%>%
  group_by(var)%>%
  summarise(mean = mean(value),
            sd = sd(value),
            p_95 = quantile(value, .95),
            p_05 = quantile(value, .05))
}

# Defining Helper function to iteratively update priors using posterior summary tables
prep_priors<-function(post_summary){
  
  # Creating necessary dataframe for brms prior construction
  prior_df<-post_summary%>%
  transmute(prior = paste0("normal(", mean, ",", sd, ")"),
         class = if_else(str_detect(var, "Intercept"), "Intercept", "b"),
         coef = if_else(str_detect(var, "Intercept"), "", str_remove(var, "^b_")))
  
  # applying beta_df
  beta_df<-prior_df%>%
    filter(class != "Intercept")

  intercept_df<-prior_df%>%
    filter(class == "Intercept")

  new_priors<-empty_prior()

  for(i in 1:nrow(beta_df)){
    new_priors[i,]<- prior_string(beta_df$prior[i], class = beta_df$class[i], coef = beta_df$coef[i])
  }

  new_priors[nrow(prior_df),]<- prior_string(intercept_df$prior[1], class = intercept_df$class[1])
  
  new_priors
}

# Linear combination of predictors
v<-function(X, beta){
  v<-X%*%beta
  v
}

# Estimated Probability through sigmoid transformation
p_hat<-function(v){

  p<-1/(1+exp(-v))
  
  p
}

```


Let's begin the model building process by defining our priors. In our problem set up, we are implementing a new system so for now we will apply a diffuse prior on each coefficient. For the sake of simplicity (and capitalizing because of the central limit theorem), I am assuming that the intercept and regression weights are normally distributed. Kind of a cool sidenote, this prior specification is actually identical to ridge regression and acts as a regulization method.

```{r}
library(brms)
library(tidybayes)
library(gganimate)

# Setting Priors
m1priors <- c(
  prior(normal(0,3), class = "Intercept"),
  prior(normal(0,3), class = "b", coef = "flexibility"),
  prior(normal(0,3), class = "b", coef = "job_alt"),
  prior(normal(0,3), class = "b", coef = "salary"),
  prior(normal(0,3), class = "b", coef = "difficulty"),
  prior(normal(0,3), class = "b", coef = "market_sal")
)
```

The model results are interesting and depict quite a bit of uncertainty in our estimates. In fact we are not sure if many of the independent variables are positively or negatively related to job acceptance. 

```{r}
# Running model
m1 <- brms::brm(
  decision ~ salary+job_alt+difficulty+flexibility+market_sal,
  data = dec,
  prior = m1priors,
  family = "bernoulli",
  seed = 123 # Adding a seed makes results reproducible.
)

# Printing output
summary(m1)

post<-post_summary(m1)

post%>%
  ggplot(aes(x = var, y = mean))+
  geom_bar(stat = "identity")+
  geom_errorbar(aes(ymin = p_05, 
                    ymax = p_95),
                width = .3)+
  geom_point(data = fe_data, aes(x = var, y = value))+
  labs(x = "", y = "Estimate", 
       title = "Posterior Estimates of Logistic Regression Coefficients", 
       subtitle = "Points represent simulation parameters.")+
  theme(panel.grid = element_blank())
```


  Below I implement an iterative update procedure over 10 data collection points to illustrate how bayesian learning can be beneficial. In short this simulates how bayesian models can be updated as new information comes in. After each iteration, I save the mean aposteriori estimate and use that as the prior for the next wave of data. I also calculate the regret or cost my model faces at each time point which is given by the difference between the predicted and observed outcome. This provides an indication of how well the model is learning. If the cost is decreasing as time increases, the model is learning!

```{r, message = FALSE, warning=FALSE}

# Initializing parameters -----------------------------------------------------------------------
# Initalizing priors for first time point
# This initializes the same priors as above for =but formats them so they can be updated in the loop
post<-data.frame(var = paste0("b_", post$var),
                 mean = rep(0, length(post$var)),
                 sd = rep(3, length(post$var)))

# initialized relevant objects for manipulation and storage
f<-decision ~ salary+job_alt+difficulty+flexibility+market_sal
y_name<-"decision"
regret<-c()
post_list<-list()

## Beginning for loop
for (j in 1:10){
  
# Solving for regret ---------------------------------------------------------------------------
  X<-model.matrix(f, data_list[[j]])
  y<-data_list[[j]][y_name]%>%
    as.matrix()%>%
    as.vector()
  
  beta<-as.matrix(post$mean)[c(3, 6, 4, 1, 2, 5),]
  v_hat<-v(X, beta)
  p<-p_hat(v_hat)
  
 regret[j]<- sum(abs(y-p))
  
  
# Set Priors -----------------------------------------------------------------------------------
new_priors<-prep_priors(post)

# Run model
new_model <- brms::brm(
  f,
  data = data_list[[j]],
  prior = new_priors,
  family = "bernoulli",
  seed = 123 # Adding a seed makes results reproducible.
)

  post<-post_summary(new_model)

  post_list[[j]]<-post
}
```

```{r, message = FALSE}
post_df<-map_dfr(1:length(post_list), function(x){
    post_list[[x]]%>%mutate(iteration = x)
    })

post_df%>%
  ggplot(aes(x = var, y = mean))+
  geom_bar(stat = "identity")+
  geom_errorbar(aes(ymin = p_05, 
                    ymax = p_95),
                width = .3)+
  geom_point(data = fe_data, aes(x = var, y = value))+
  labs(x = "", y = "Estimate", 
       title = "Posterior Estimates of Logistic Regression Coefficients at Time {closest_state} of 10", 
       subtitle = "Points represent simulation parameters.")+
  theme(panel.grid = element_blank())+
  transition_states(iteration,
                    transition_length = 2,
                    state_length = 1)

ggplot(data = NULL, aes(x = 1:length(regret), y = regret))+
  geom_line()+
  labs(x = "", y = "Cost", 
       title = "Cost Incurred at Time t")+
  theme(panel.grid = element_blank())+
 scale_x_continuous(breaks = 1:10)


```


### LaPlace Approximation

While MCMC is super flexible, it isn't the most scalable solution. This is because it is rather slow - especially implemented through `brms` which requires the compilation of a c++ model. The primary problem with bayesian logistic regression is that the we can't solve analytically for $p(D)$. A work around is to solve for the mean aposterior given normal priors and than approximate the distribution with a taylor expansion (covered here in the context of optimization) about the estimate. 

How do we solve for the mean aposeriori estimate? This is done my maximizing the joint likelihood: 

$$p(D|\beta)p(\beta|\beta_{prior})$$

$P(D|\beta)$ is simply the typical likelihood for binomial random variables. 

$$L(\beta) = -\sum_{i=1}^n\ y_i\ ln(\ \hat p_i(y = 1)\ )+(1-y_i)\ ln(\ 1-\hat p_i(y = 1)\ )$$
$p(\beta|\beta_{prior})$ can be solved simply using the multivariate normal desnity. This means that in order to solve for the mean a posteriori we need to

$$\min_\theta -(\sum_{i=1}^n\ y_i\ ln(\ \hat p_i(y = 1)\ )+(1-y_i)\ ln(\ 1-\hat p_i(y = 1)\ ))+log((\theta-\mu)\Sigma^{-1}(\theta-\mu)))$$
```{r}
# Defining Objective Function ----------------------------------------------------------------
post_ll<-function(theta, X, y, prior_beta, prior_vcov){
 
  v_hat<-v(X = X, beta = theta) 
  p<-p_hat(v_hat)
  
  ll_x<--sum(y*log(p)+(1-y)*log(1-p))
  ll_p<--log(mixtools::dmvnorm(y = theta-prior_beta, mu = rep(0, length(theta)), sigma = prior_vcov))
 
  ll_post<- ll_x+ll_p

  ll_post
}

```

  The taylor expansion has some cool properties. First, by minimizing the joint likelihood with respect to theta, we set the first derivative equal to 0. This means that the first order term drops out leaving the taylor expansion with only the inverse hessian. Since derivatives are additive we can chunk the objective function to solve for the hessian. The first part of the equation is composed of the typical likelihood function for a binomial random variable. We solved for the hessian in the first post. The second part is the multivariate normal density which has a well known hessian ($\Sigma^{-1}$). We can simply add the covariance matrix of the priors to the hessian of the likelihood to solve for the taylor expansion. 

```{r}
# Implementation of taylor expansions
logistic_laplace<-function(formula=NULL, data=NULL, prior_beta=NULL, prior_vcov=NULL, X, y){
  
  # Prepping requisite objects -----------------------------------------------------------
  if(!is.null(formula) & !is.null(data)){
    X<-model.matrix(formula, data)
    y<-as.vector(as.matrix(data[as.character(formula)[2]]))
  }
  
  p<-ncol(X)
  
  # Handling priors if non are provided ---------------------------------------------------
  if(is.null(prior_beta)){
    prior_beta<-rep(0, p)
  }
  
  if(is.null(prior_vcov)){
    prior_vcov<-diag(5, nrow = p, ncol= p)
  }
  
  # Solving for theta --------------------------------------------------------------------------
    # using optim is cheating, I know
    # Solution for the hessian is outlined above and the derivatives are pretty easy
    # I think the motivated person would be able to implement newtonian optimization without me spelling it out
   rs<-optim(par = prior_beta, post_ll, hessian = TRUE, prior_beta = prior_beta, prior_vcov = prior_vcov, X = X, y = y)
  
   post_beta<-rs$par
   
   post_cov<-solve(rs$hessian)
   
   list(MAP = post_beta, 
        vcov = post_cov)
}

# Helper function for posterior extraction from lists

laplace_posterior<-function(model_list, coef_names){
  vcov_list<-map(model_list, "vcov")
  coef_list<-map(model_list, "MAP")
  
  
  vcov<-map_dfr(vcov_list, function(x){
                                   variance<- diag(x)
                                   names(variance)<-coef_names
                                   variance<-t(variance)
                                   as.data.frame(variance)}
          )%>%
  mutate(Time = 1:n())%>%
  gather(key = "Coefficient",
             value = "VAR", 
             - Time)


map<-map_dfr(coef_list, function(x){names(x)<-coef_names
                                   x<-t(x)
                                   as.data.frame(x)}
        )%>%
      mutate(Time = 1:n())%>%
      gather(key = "Coefficient",
             value = "MAP", 
             - Time)

posterior_dist<-left_join(map, vcov)

posterior_dist
}

```

  When do Laplace approximations work? The taylor expansion implies that the posterior distribution will me normal. They work well in scearios for which a normal posterior would be appropriate. Given that a normal posterior is typically appriate for GLM models when the sample size is sufficiently large, this is often a good candidate for the priors. Below, I am going to replicate the analysis done above, but then explore some slight variations that weren't possible with `brms` because of run-time restrictions. In the first variation, I am going to update the models with smaller batches (but the same overall sample size n = 500). Second, I am going to explore how the model performs when the simulation parameters are not stationary. 

```{r, echo = FALSE}
iterative_gen_decisions<-function(fixed_effects, n_people){
  # Simulate People ------------------------------------------------------------------  
# Participants have their own preferences associated with each of these positons, for now we will simulate them as being fixed (i.e., an single preference parameter adequetly summarises all people)
# We simulate these preference distributions below. 
people<-suppressMessages(sim_people(fixed_effects = fixed_effects, n_people = n_people, n_blocks = 50, p_blocks = rep(.02, 50)))


#########################################
######## Simulating decisions ###########
#########################################
# The following code chunch has the simulated people (i.e., job applicants) choose between accepting and rejecting the simulated job offers
# I then organize the data frame a bit more so that the meaning of variables is clear

dec<-suppressMessages(sim_dec(design = design_df, people = people))

dec<-dec%>%
  ungroup()%>%
  filter(alt == 1)%>%
  select(job_id = BLOCK, applicant_id = id, salary = cost,  job_alt:market_sal, decision)

dec
}
```

#### Simulation 1: 50 Observation Batches with Deterministic Data Generation

```{r}



# Define initial priors
prior_beta<-rep(0, 6)
prior_vcov<-diag(3, 6)

# Initialize storage and useful objects
model_list<-list()
loss<-vector("numeric")

f<-decision~salary+job_alt+flexibility+difficulty+market_sal
y_name<-"decision"

for(i in 1:10){

  # Solving for the cost -----------------------------
    X<-model.matrix(f, data_list[[i]])
    y<-as.vector(as.matrix(data_list[[i]][y_name]))
    v_hat<-v(X, prior_beta)
    p<-p_hat(v_hat)
    loss[[i]]<-sum(abs(y-p))
  
  # Running model -------------------------------------
  m<-logistic_laplace(f, 
                      data = data_list[[i]], 
                      prior_beta = prior_beta, 
                      prior_vcov = prior_vcov)
  
  # Updating Priors -----------------------------------
  prior_beta<-m$MAP
  prior_vcov<-m$vcov
  
  # Storing model output ------------------------------
  model_list[[i]]<-m
}

# Defining coefficient names
coef_names<-c("Intercept", "Salary", "job_alt", "flexibility", "difficulty", "market_sal")

post_summary<-laplace_posterior(model_list = model_list, coef_names = coef_names)

post_summary%>%
  ggplot(aes(x = Coefficient, y = MAP))+
  geom_bar(stat = "identity")+
  geom_errorbar(aes(ymin = MAP-2*sqrt(VAR), 
                    ymax = MAP+2*sqrt(VAR)),
                width = .3)+
  geom_point(data = fe_data, aes(x = var, y = value))+
  labs(x = "", y = "Estimate", 
       title = "Posterior Estimates of Logistic Regression Coefficients at Time {closest_state} of 10", 
       subtitle = "Points represent simulation parameters.")+
  theme(panel.grid = element_blank())+
  transition_states(Time,
                    transition_length = 2,
                    state_length = 1)


ggplot(data = NULL, aes(x = 1:length(loss), y = loss))+
  geom_line()+
  labs(x = "", y = "Cost", 
       title = "Cost Incurred at Time t")+
  theme(panel.grid = element_blank())+
 scale_x_continuous(breaks = 1:10)

 
```

#### Simulation 2: 5 Observation Batches with Deterministic Data Generation

```{r, echo = FALSE}
data_list<-map(1:100, function(x){
  iterative_gen_decisions(fixed_effects = fe, n_people = 5)
})
```


```{r}
# Define initial priors
prior_beta<-rep(0, 6)
prior_vcov<-diag(3, 6)

# Initialize storage and useful objects
model_list<-list()
loss<-vector("numeric")

f<-decision~salary+job_alt+flexibility+difficulty+market_sal
y_name<-"decision"

for(i in 1:100){

  # Solving for the cost -----------------------------
    X<-model.matrix(f, data_list[[i]])
    y<-as.vector(as.matrix(data_list[[i]][y_name]))
    v_hat<-v(X, prior_beta)
    p<-p_hat(v_hat)
    loss[[i]]<-sum(abs(y-p))
  
  # Running model -------------------------------------
  m<-logistic_laplace(f, 
                      data = data_list[[i]], 
                      prior_beta = prior_beta, 
                      prior_vcov = prior_vcov)
  
  # Updating Priors -----------------------------------
  prior_beta<-m$MAP
  prior_vcov<-m$vcov
  
  # Storing model output ------------------------------
  model_list[[i]]<-m
}

# Defining coefficient names
coef_names<-c("Intercept", "salary", "job_alt", "flexibility", "difficulty", "market_sal")


post_summary<-laplace_posterior(model_list = model_list, coef_names)

post_summary%>%
  ggplot(aes(x = Coefficient, y = MAP))+
  geom_bar(stat = "identity")+
  geom_errorbar(aes(ymin = MAP-2*sqrt(VAR), 
                    ymax = MAP+2*sqrt(VAR)),
                width = .3)+
  geom_point(data = fe_data, aes(x = var, y = value))+
  labs(x = "", y = "Estimate", 
       title = "Posterior Estimates of Logistic Regression Coefficients at Time {closest_state} of 100", 
       subtitle = "Points represent simulation parameters.")+
  theme(panel.grid = element_blank())+
  transition_states(Time,
                    transition_length = 2,
                    state_length = 1)


ggplot(data = NULL, aes(x = 1:length(loss), y = loss))+
  geom_line()+
  labs(x = "", y = "Cost", 
       title = "Cost Incurred at Time t")+
  theme(panel.grid = element_blank())+
 scale_x_continuous(breaks = 1:100)
```
```{r, echo = FALSE}

fe_data1<-fe_data%>%
  expand_grid(Time = 1:33)
fe_data2<-fe_data%>%
  mutate(value = value + rnorm(6, -1, .3))%>%
  expand_grid(Time = 34:66)
fe_data3<-fe_data%>%
  mutate(value = value + rnorm(6, 2, .3))%>%
  expand_grid(Time = 67:100)

non_stat<-bind_rows(fe_data1, fe_data2, fe_data3)

data_list<-map(1:100, function(x){
 tmp<- non_stat%>%
    filter(Time == x)%>%
    pull("value")
 
 names(tmp)<-names(fe)
  iterative_gen_decisions(fixed_effects = tmp, n_people = 5)
})
```


```{r}
# Define initial priors
prior_beta<-rep(0, 6)
prior_vcov<-diag(3, 6)

# Initialize storage and useful objects
model_list<-list()
loss<-vector("numeric")

f<-decision~salary+job_alt+flexibility+difficulty+market_sal
y_name<-"decision"

for(i in 1:100){

  # Solving for the cost -----------------------------
    X<-model.matrix(f, data_list[[i]])
    y<-as.vector(as.matrix(data_list[[i]][y_name]))
    v_hat<-v(X, prior_beta)
    p<-p_hat(v_hat)
    loss[[i]]<-sum(abs(y-p))
  
  # Running model -------------------------------------
  m<-logistic_laplace(f, 
                      data = data_list[[i]], 
                      prior_beta = prior_beta, 
                      prior_vcov = prior_vcov)
  
  # Updating Priors -----------------------------------
  prior_beta<-m$MAP
  prior_vcov<-m$vcov
  
  # Storing model output ------------------------------
  model_list[[i]]<-m
}

# Defining coefficient names
coef_names<-c("Intercept", "salary", "job_alt", "flexibility", "difficulty", "market_sal")


post_summary<-laplace_posterior(model_list = model_list, coef_names)

post_summary%>%
  ggplot(aes(x = Coefficient, y = MAP))+
  geom_bar(stat = "identity")+
  geom_errorbar(aes(ymin = MAP-2*sqrt(VAR), 
                    ymax = MAP+2*sqrt(VAR)),
                width = .3)+
  geom_point(data = non_stat, aes(x = var, y = value))+
  labs(x = "", y = "Estimate", 
       title = "Posterior Estimates of Logistic Regression Coefficients at Time {closest_state} of 100", 
       subtitle = "Points represent simulation parameters.")+
  theme(panel.grid = element_blank())+
  transition_states(Time,
                    transition_length = 2,
                    state_length = 1)


ggplot(data = NULL, aes(x = 1:length(loss), y = loss))+
  geom_line()+
  labs(x = "", y = "Cost", 
       title = "Cost Incurred at Time t")+
  theme(panel.grid = element_blank())+
 scale_x_continuous(breaks = 1:100)
```


## Some great posts here
[here](https://www.countbayesie.com/blog/2019/6/12/logistic-regression-from-bayes-theorem) and a more applied perspective [here](https://www.jamesrrae.com/post/bayesian-logistic-regression-using-brms-part-1/)